{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fd810e-8edc-4eab-9ee6-b89206f0e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f52r714/.conda/envs/lang_4/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from transformers import AutoModel\n",
    "from time import perf_counter as timer\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from is_tpt_ref import ReferenceClassifier\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load dataframe\n",
    "text_chunks_and_embeddings_df = pd.read_pickle(\"ajp_perc_prper_tpt_text_chunks_embeddings_jinav3_5.pkl\")\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "doi_and_chunks = text_chunks_and_embeddings_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Load embeddings onto GPU\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embeddings_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "\n",
    "# Load model\n",
    "embedding_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True, device_map=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b457b7f8-30c6-4114-88f4-c14fc38f7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Clean the labels on the data\n",
    "csv=pd.read_excel(\"paper_labels_M_S_2.xlsx\")\n",
    "\n",
    "csv['Category (M)']=csv['Category (M)'].str.replace(\"*\", \"\").replace(\"Journal business\", \"journal business\").replace(\"Teacher\", \"teacher\").replace('teaching', 'teacher').replace('Content', 'content').replace('content  ', 'content').replace('Student', 'student').replace(\"Content    \", \"content\").replace(\"Content  \", \"content\").replace(\"Teacher  \", \"teacher\").replace(\"Teacher \", \"teacher\").replace(\"Teaching\", \"teacher\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279897e9-ad69-460b-bb39-60de9634e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ['Teaching students.', 'Student focus.', 'Physics content.', 'Journal business.']\n",
      "Time take to get scores on 1180219 embeddings: 0.25976 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Define the title of the groups, used for quick/human reading\n",
    "t1 = \"Teaching students.\"\n",
    "t2 = \"Student focus.\"\n",
    "t3 = \"Physics content.\"\n",
    "t4= \"Journal business.\"\n",
    "\n",
    "#Make the list of queries titles. \n",
    "query_list = [t1, t2, t3, t4]\n",
    "\n",
    "#Describe the content of each group. This is the actual text that the \"user\" would enter to separate the papers\n",
    "s1=\"Teaching. Laboratory equipment. Teaching methods.\"\n",
    "s2=\"Student belonging. Student focused. Student agency.\"\n",
    "s3=\"Physics content. Physics material. Math. Derivations.\"\n",
    "s4=\"Editorials, book reviews, announcements, obituaries. Journal business. Reports on business. \"\n",
    "\n",
    "#Put them into a list\n",
    "query_list_verbose=[s1, s2, s3, s4]\n",
    "query_embedding = embedding_model.encode(query_list_verbose, convert_to_tensor=True)\n",
    "\n",
    "print(f\"Query: {query_list}\")\n",
    "\n",
    "# Compute dot product similarity\n",
    "start_time = timer()\n",
    "dot_scores_1 = util.dot_score(a=query_embedding, b=embeddings).T  # Shape: (n_embeddings, 3)\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# Add columns for s1, s2, s3, s4 similarity to dataframe\n",
    "for n, t in enumerate(query_list):\n",
    "    text_chunks_and_embeddings_df[t]= dot_scores_1[:, n].cpu().numpy() #Loaded into the CPU at first because that's how the tensors are loaded in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d344c1-2cfd-4472-8d43-9c7ae6f7194b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e0dda7-29a0-4d9c-a81b-96eb0b385d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "##But first we're going to calculate the topic scores for sentence chunk. For more on this function, see Odden et al. (2024)\n",
    "\n",
    "# Default value of 'a'\n",
    "a = -10\n",
    "\n",
    "# Precompute the exponentials for efficiency\n",
    "exp_values = np.exp(a * (1-text_chunks_and_embeddings_df[query_list]))\n",
    "\n",
    "# Compute the denominator for softmax-like normalization\n",
    "denominator = exp_values.sum(axis=1)\n",
    "\n",
    "# Create new score columns\n",
    "\n",
    "for col in query_list:\n",
    "    score_col = f\"{col.strip()}_score\"\n",
    "    text_chunks_and_embeddings_df[score_col] = exp_values[col] / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1757a-f669-447a-a460-0c4f9e6db8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a595f5a5-ac6f-46b8-a9a9-67e1f5c32ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_575724/4228234821.py:110: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_weighted = df.groupby(\"doi\").apply(compute_weighted_scores).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_weighted keys\n",
      "Index(['doi', 'Teaching students._weighted',\n",
      "       'Teaching students._score_weighted', 'Student focus._weighted',\n",
      "       'Student focus._score_weighted', 'Physics content._weighted',\n",
      "       'Physics content._score_weighted', 'Journal business._weighted',\n",
      "       'Journal business._score_weighted'],\n",
      "      dtype='object')\n",
      "Time it took to complete the averaging: 17.305710554122925\n",
      "Time it took to complete weighted centroiding: 2.4801182746887207\n",
      "test1\n",
      "Time to add weighted centroids to grouped_weighted: 0.012016057968139648\n",
      "Here \n",
      "\n",
      "\n",
      "Index(['doi', 'Teaching students._weighted',\n",
      "       'Teaching students._score_weighted', 'Student focus._weighted',\n",
      "       'Student focus._score_weighted', 'Physics content._weighted',\n",
      "       'Physics content._score_weighted', 'Journal business._weighted',\n",
      "       'Journal business._score_weighted', 'Weighted Centroid', 'Centroid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Weight mask was a concept that was initially oging to be used to cut out sections of the text we found to be unhelpful\n",
    "# This never ended up being used, but we did code it to format our data so it needs to remain in. The weights should work if \n",
    "# you ever want to use it. The format is simple, [0, 50, 200, 100] means the first sentence chunk is not considered, the second chunk is \n",
    "# weighted half, the tird chunk is weighted twice as much as everything else, and the fourth chunk is weighted as much as the rest of the paper\n",
    "def apply_weight_mask(df, query_list, chunk_weights):\n",
    "    \"\"\"\n",
    "    Applies weighted averaging to chunks (sentence blocks) for each DOI and computes weighted embeddings.\n",
    "    \n",
    "    This function processes a DataFrame containing document chunks grouped by DOI, applying \n",
    "    specified weights to each chunk position across all DOIs. It computes weighted averages\n",
    "    for query-related columns and creates weighted centroid embeddings for each DOI.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing document chunks with the following expected columns:\n",
    "        - 'doi': Document identifier for grouping chunks\n",
    "        - 'embedding': PyTorch tensor representing the embedding for each chunk\n",
    "        - Query columns specified in query_list (e.g., similarity scores)\n",
    "        - Corresponding '_score' columns for each query column\n",
    "        - Optional 'weight' column (will be created if not present)\n",
    "    \n",
    "    query_list : list of str\n",
    "        List of column names representing query-related metrics to be weighted.\n",
    "        The function expects corresponding '{col}_score' columns to exist in the DataFrame.\n",
    "    \n",
    "    chunk_weights : list of float\n",
    "        List of weight values to apply to chunks, where:\n",
    "        - Each value corresponds to the weight for a chunk at that index position\n",
    "        - Length determines how many chunk positions receive custom weights\n",
    "        - Values are normalized by dividing by 100.0\n",
    "        - Chunks beyond this length retain default weight of 1.0\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Processed DataFrame with the following columns:\n",
    "        - 'doi': Document identifier\n",
    "        - '{col}_weighted': Weighted average for each query column\n",
    "        - '{col}_score_weighted': Weighted average for each query score column  \n",
    "        - 'Weighted Centroid': PyTorch tensor representing weighted centroid embedding\n",
    "        - 'Centroid': PyTorch tensor representing unweighted mean embedding\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Chunks are indexed by their order within each DOI group (0-indexed)\n",
    "    - Weights are normalized by dividing input values by 100.0\n",
    "    - For embeddings, weighted centroids are computed as: sum(embedding * weight) / sum(weights)\n",
    "    - Unweighted centroids are computed as simple mean of all embeddings per DOI\n",
    "    - Function includes timing prints for performance monitoring\n",
    "    - Requires PyTorch for tensor operations\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> import torch\n",
    "    >>> \n",
    "    >>> # Sample data\n",
    "    >>> df = pd.DataFrame({\n",
    "    ...     'doi': ['10.1000/1', '10.1000/1', '10.1000/2'],\n",
    "    ...     'query1': [0.8, 0.6, 0.9],\n",
    "    ...     'query1_score': [0.85, 0.65, 0.95],\n",
    "    ...     'embedding': [torch.randn(128), torch.randn(128), torch.randn(128)]\n",
    "    ... })\n",
    "    >>> \n",
    "    >>> result = apply_weight_mask(df, ['query1'], [80, 60])\n",
    "    >>> print(result.columns)\n",
    "    Index(['doi', 'query1_weighted', 'query1_score_weighted', \n",
    "           'Weighted Centroid', 'Centroid'], dtype='object')\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "        If expected columns (doi, embedding, or query columns) are missing from input DataFrame\n",
    "    IndexError  \n",
    "        If chunk_weights list is empty\n",
    "    ValueError\n",
    "        If embeddings cannot be stacked (inconsistent tensor dimensions)\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize weight values to match the scale of the weight system\n",
    "    normalized_weights = [weight / 100.0 for weight in chunk_weights]\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[\"chunk_index\"] = df.groupby(\"doi\").cumcount()\n",
    "\n",
    "    # Create a weight column\n",
    "    if \"weight\" not in df.columns:\n",
    "        df[\"weight\"] = 1.0  # Default weight for non-specified chunks\n",
    "    \n",
    "    # Apply the provided weights to each chunk index\n",
    "    max_chunk_index = len(chunk_weights)\n",
    "    for idx in range(min(max_chunk_index, df[\"chunk_index\"].max() + 1)):\n",
    "        df.loc[df[\"chunk_index\"] == idx, \"weight\"] = normalized_weights[idx]\n",
    "    \n",
    "    # Multiply each of the query columns by the weight\n",
    "    for col in query_list:\n",
    "        df[col + \"_weighted\"] = df[col] * df[\"weight\"]\n",
    "    for col in query_list:\n",
    "        df[col + \"_score_weighted\"] = df[f\"{col}_score\"] * df[\"weight\"]\n",
    "\n",
    "    ##Helper function to compute the similarity scores given the inputted weights\n",
    "    def compute_weighted_scores(g):\n",
    "        result = {}\n",
    "        for col in query_list:\n",
    "            result[f\"{col}_weighted\"] = g[col + \"_weighted\"].sum() / g[\"weight\"].sum()\n",
    "            result[f\"{col}_score_weighted\"] = g[col + \"_score_weighted\"].sum() / g[\"weight\"].sum()\n",
    "        return pd.Series(result)\n",
    "\n",
    "    grouped_weighted = df.groupby(\"doi\").apply(compute_weighted_scores).reset_index()\n",
    "    print(\"grouped_weighted keys\")\n",
    "    print(grouped_weighted.keys())\n",
    "    # Step 1: Apply weighting to embeddings\n",
    "    df[\"weighted_embedding\"] = df.apply(lambda row: row[\"embedding\"] * row[\"weight\"], axis=1)\n",
    "    # Step 2: Group manually and compute sums\n",
    "    grouped_rows = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for doi, group in df.groupby(\"doi\"):\n",
    "        embeddings = group[\"weighted_embedding\"].tolist()\n",
    "        weights = group[\"weight\"].tolist()\n",
    "        emb_unweighted= group['embedding'].tolist()\n",
    "        \n",
    "        summed_embedding = torch.stack(embeddings).sum(dim=0)\n",
    "        total_weight = sum(weights)\n",
    "        mean_embedding= torch.stack(emb_unweighted).mean(dim=0)\n",
    "        grouped_rows.append({\n",
    "            \"doi\": doi,\n",
    "            \"weighted_embedding\": summed_embedding,\n",
    "            \"weight\": total_weight,\n",
    "            \"unweighted_centroid\": mean_embedding\n",
    "        })\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Time it took to complete the averaging: {end_time-start_time}\")\n",
    "    \n",
    "    grouped_df = pd.DataFrame(grouped_rows)\n",
    "    start=time.time()\n",
    "    # Step 3: Compute weighted centroids\n",
    "    grouped_df[\"Weighted Centroid\"] = grouped_df.apply(\n",
    "        lambda row: row[\"weighted_embedding\"] / row[\"weight\"]\n",
    "        if row[\"weight\"] != 0 else torch.zeros_like(row[\"weighted_embedding\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    end=time.time()\n",
    "    print(f\"Time it took to complete weighted centroiding: {end-start}\")\n",
    "    print(\"test1\")\n",
    "    #print(grouped_rows)\n",
    "\n",
    "    start=time.time()\n",
    "    \n",
    "    # Merge to add the 'Weighted Centroid' to grouped_weighted\n",
    "    grouped_weighted = grouped_weighted.merge(\n",
    "        grouped_df[[\"doi\", \"Weighted Centroid\", \"unweighted_centroid\"]],\n",
    "        on=\"doi\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    end=time.time()\n",
    "    grouped_weighted=grouped_weighted.rename(columns={'unweighted_centroid':\"Centroid\"})\n",
    "    print(f\"Time to add weighted centroids to grouped_weighted: {end-start}\")\n",
    "    return grouped_weighted\n",
    "# Apply weighting. The following implies that the first three sentence chunks are to be weighted equally (no weighting). \n",
    "weight = [100, 100, 100 ]         \n",
    "\n",
    "# Apply the functions\n",
    "df=text_chunks_and_embeddings_df\n",
    "grouped = apply_weight_mask(df, query_list, weight)\n",
    "\n",
    "#Make a new column for the topic scores if weighting is applied. \n",
    "weighted_ql=[]\n",
    "for query in query_list:\n",
    "    weighted_ql.append(f\"{query}_weighted\")\n",
    "\n",
    "\n",
    "# Determine dominant group, insert that label as a new column\n",
    "grouped[\"Main Group DotP Weighted\"] = grouped[weighted_ql].idxmax(axis=1)\n",
    "\n",
    "\n",
    "#This grabs items like \"title\", \"doi\", \"Journal\", and \"year\" from the dataframe and adds that relevant information into\n",
    "#the dataframe \"grouped\". \n",
    "t=df.drop_duplicates('doi', keep='last').reset_index()\n",
    "t2=t[['title','doi', 'journal', 'year']]\n",
    "\n",
    "grouped=t2.merge(grouped, how='right', on='doi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2c65f2-33dd-4b7f-9d7a-cedd696fc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabs labels form the CSV and inserts them into the dataframe. Renames the dataframe df_labels. \n",
    "\n",
    "weighted_score_ql=[]\n",
    "for query in query_list:\n",
    "    weighted_score_ql.append(f\"{query}_score_weighted\")\n",
    "\n",
    "\n",
    "csv=csv.rename(columns={\"Doi\": \"doi\"})\n",
    "csv['doi']=csv['doi'].str.replace(\" \", \"\")\n",
    "csv['doi']=csv['doi'].str.replace(\"\\'\", \"\")\n",
    "csv['doi']=csv['doi'].str.replace(\"https://doi.org/\", \"\")\n",
    "t=grouped[[\"doi\", \"year\", \"title\", \"Weighted Centroid\", \"Centroid\", \"journal\" ] + weighted_ql + weighted_score_ql]\n",
    "t_=t.reset_index()\n",
    "df_labels=pd.merge(t_, csv[['Category (M)', 'doi']], on='doi', how=\"left\")\n",
    "\n",
    "##At this point, the dataframe has handmade labels for the items that have received them by matching DOI. The labels are in single letter format\n",
    "##the main topic labels need to be converted to this format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674feffd-3996-4e74-a955-acb507559855",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calcualting the topic score after weights have been applied. \n",
    "\n",
    "# Default value of 'a'\n",
    "#a = -10\n",
    "\n",
    "# Precompute the exponentials for efficiency\n",
    "exp_values = np.exp(a * (1-df_labels[weighted_ql]))\n",
    "\n",
    "# Compute the denominator for softmax-like normalization\n",
    "denominator = exp_values.sum(axis=1)\n",
    "\n",
    "# Create new score columns\n",
    "\n",
    "for col in weighted_ql:\n",
    "    score_col = f\"{col.strip()}_score_avg_cos\"\n",
    "    df_labels[score_col] = exp_values[col] / denominator\n",
    "\n",
    "# Step 2: Get main_group column based on max score\n",
    "score_cols = [f\"{col.strip()}_score_avg_cos\" for col in weighted_ql]\n",
    "df_labels['MG Score Avg Cos'] = df_labels[score_cols].idxmax(axis=1).str.replace('_weighted_score_avg_cos', '')\n",
    "\n",
    "# Step 3: Define evaluator function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59fa25b-b4ea-4035-b616-33c5fc32d851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc834b6-7bc8-4b2c-b19a-7adcf756d106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940e7db2-9fe7-4c5d-80bb-e8e1d283633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the topic score applied to each paper by averaging together the cosine similarities \n",
    "\n",
    "#a = -10\n",
    "\n",
    "\n",
    "# Create new score columns\n",
    "#score_cols = [f\"{col.strip()}_score_weighted\" for col in weighted_ql]\n",
    "df_labels['MG Score Avg Score'] = df_labels[weighted_ql].idxmax(axis=1).str.replace('_weighted', '')\n",
    "\n",
    "\n",
    "# Display the updated dataframe structure\n",
    "#print(df_labels.head())\n",
    "\n",
    "##Want to also calculate here the labels based on assigning a topic score to each sentence chunk, then averaging that\n",
    "##and seeing which one of those works better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c24b7d-729c-45a0-b3fe-99bf75c80ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4369f5-e872-4b79-aa1b-b53928d824a3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for n,  (cos, score) in enumerate(zip(df_labels['MG Score Avg Cos'], df_labels['MG Score Avg Cos'])):\n",
    "    same_val= cos==score\n",
    "    if same_val==False:\n",
    "        print(f\"{n}: {cos} {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50f2aa-89f2-4265-9302-3f77a09ea8ea",
   "metadata": {},
   "source": [
    "for element, title in zip(df_labels.iloc[766], df_labels.keys()):\n",
    "    print(f\"label: {title} Element: {element}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898b7dc-6b16-4700-8bc2-211d946932dd",
   "metadata": {},
   "source": [
    "grouped[grouped['doi']=='10.1103/PhysRevPhysEducRes.13.019901']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3473804d-5329-4017-833f-7153f3f976e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in file with embeddings of the whole paper pre-computed\n",
    "full_text_and_embeddings = pd.read_pickle(\"ajp_perc_prper_tpt_full_text_embeddings_2.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cc803c1-ddc3-4c7b-a162-74b3cc111442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>doi</th>\n",
       "      <th>year</th>\n",
       "      <th>journal</th>\n",
       "      <th>char_count</th>\n",
       "      <th>token_count</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentences_count</th>\n",
       "      <th>sentence_chunks</th>\n",
       "      <th>num_chunks</th>\n",
       "      <th>Full Text Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make a Mystery Circuit with a Bar Light Fixtur...</td>\n",
       "      <td>10.1119/1.2715425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tpt</td>\n",
       "      <td>7234</td>\n",
       "      <td>1808.50</td>\n",
       "      <td>[Make a Mystery Circuit with a Bar Light Fixtu...</td>\n",
       "      <td>81</td>\n",
       "      <td>[[Make a Mystery Circuit with a Bar Light Fixt...</td>\n",
       "      <td>17</td>\n",
       "      <td>[tensor(-0.0012), tensor(0.0354), tensor(0.108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGOLDEN OLDIE-ABLAOK BOX OIROUIT \\r\\nClifton K...</td>\n",
       "      <td>10.1119/1.2343976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tpt</td>\n",
       "      <td>4633</td>\n",
       "      <td>1158.25</td>\n",
       "      <td>[AGOLDEN OLDIE-ABLAOK BOX OIROUIT \\r\\nClifton ...</td>\n",
       "      <td>58</td>\n",
       "      <td>[[AGOLDEN OLDIE-ABLAOK BOX OIROUIT \\r\\nClifton...</td>\n",
       "      <td>12</td>\n",
       "      <td>[tensor(0.0039), tensor(-0.0006), tensor(0.114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modeling Electricity: Model-Based Inquiry with...</td>\n",
       "      <td>10.1119/1.4745686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tpt</td>\n",
       "      <td>17496</td>\n",
       "      <td>4374.00</td>\n",
       "      <td>[Modeling Electricity: Model-Based Inquiry wit...</td>\n",
       "      <td>140</td>\n",
       "      <td>[[Modeling Electricity: Model-Based Inquiry wi...</td>\n",
       "      <td>28</td>\n",
       "      <td>[tensor(0.0564), tensor(-0.0816), tensor(0.150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two Approaches to Learning Physics \\r\\n\"I look...</td>\n",
       "      <td>10.1119/1.2342910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tpt</td>\n",
       "      <td>37322</td>\n",
       "      <td>9330.50</td>\n",
       "      <td>[Two Approaches to Learning Physics \\r\\n\"I loo...</td>\n",
       "      <td>324</td>\n",
       "      <td>[[Two Approaches to Learning Physics \\r\\n\"I lo...</td>\n",
       "      <td>65</td>\n",
       "      <td>[tensor(0.0499), tensor(-0.1132), tensor(0.178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\r\\nJochen Kuhn and Patrik Vogt, Column Editor...</td>\n",
       "      <td>10.1119/1.4865529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tpt</td>\n",
       "      <td>6621</td>\n",
       "      <td>1655.25</td>\n",
       "      <td>[\\r\\nJochen Kuhn and Patrik Vogt, Column Edito...</td>\n",
       "      <td>76</td>\n",
       "      <td>[[\\r\\nJochen Kuhn and Patrik Vogt, Column Edit...</td>\n",
       "      <td>16</td>\n",
       "      <td>[tensor(0.0292), tensor(-0.0519), tensor(0.055...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43602</th>\n",
       "      <td>Examining faculty choices while implementing t...</td>\n",
       "      <td>10.1119/perc.2023.pr.Willison</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>perc</td>\n",
       "      <td>24881</td>\n",
       "      <td>6220.25</td>\n",
       "      <td>[Examining faculty choices while implementing ...</td>\n",
       "      <td>203</td>\n",
       "      <td>[[Examining faculty choices while implementing...</td>\n",
       "      <td>41</td>\n",
       "      <td>[tensor(0.1635), tensor(-0.1263), tensor(0.057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43603</th>\n",
       "      <td>Analyzing the dimensionality of the Energy and...</td>\n",
       "      <td>10.1119/perc.2023.pr.Wu</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>perc</td>\n",
       "      <td>24460</td>\n",
       "      <td>6115.00</td>\n",
       "      <td>[Analyzing the dimensionality of the Energy an...</td>\n",
       "      <td>227</td>\n",
       "      <td>[[Analyzing the dimensionality of the Energy a...</td>\n",
       "      <td>46</td>\n",
       "      <td>[tensor(0.0763), tensor(-0.1726), tensor(0.054...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43604</th>\n",
       "      <td>Students’ use of symmetry as a tool for sensem...</td>\n",
       "      <td>10.1119/perc.2023.pr.Young</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>perc</td>\n",
       "      <td>27598</td>\n",
       "      <td>6899.50</td>\n",
       "      <td>[Students’ use of symmetry as a tool for sense...</td>\n",
       "      <td>264</td>\n",
       "      <td>[[Students’ use of symmetry as a tool for sens...</td>\n",
       "      <td>53</td>\n",
       "      <td>[tensor(0.1260), tensor(-0.0853), tensor(0.105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43605</th>\n",
       "      <td>Analyzing Physics Majors’ Specialization Low I...</td>\n",
       "      <td>10.1119/perc.2023.pr.Zohrabi_Alaee</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>perc</td>\n",
       "      <td>27954</td>\n",
       "      <td>6988.50</td>\n",
       "      <td>[Analyzing Physics Majors’ Specialization Low ...</td>\n",
       "      <td>222</td>\n",
       "      <td>[[Analyzing Physics Majors’ Specialization Low...</td>\n",
       "      <td>45</td>\n",
       "      <td>[tensor(0.0705), tensor(-0.1829), tensor(0.015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43606</th>\n",
       "      <td>Analyzing AI and student responses through the...</td>\n",
       "      <td>10.1119/perc.2023.pr.Zollman</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>perc</td>\n",
       "      <td>27691</td>\n",
       "      <td>6922.75</td>\n",
       "      <td>[Analyzing AI and student responses through th...</td>\n",
       "      <td>219</td>\n",
       "      <td>[[Analyzing AI and student responses through t...</td>\n",
       "      <td>44</td>\n",
       "      <td>[tensor(0.1471), tensor(-0.1589), tensor(0.174...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43607 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      Make a Mystery Circuit with a Bar Light Fixtur...   \n",
       "1      AGOLDEN OLDIE-ABLAOK BOX OIROUIT \\r\\nClifton K...   \n",
       "2      Modeling Electricity: Model-Based Inquiry with...   \n",
       "3      Two Approaches to Learning Physics \\r\\n\"I look...   \n",
       "4      \\r\\nJochen Kuhn and Patrik Vogt, Column Editor...   \n",
       "...                                                  ...   \n",
       "43602  Examining faculty choices while implementing t...   \n",
       "43603  Analyzing the dimensionality of the Energy and...   \n",
       "43604  Students’ use of symmetry as a tool for sensem...   \n",
       "43605  Analyzing Physics Majors’ Specialization Low I...   \n",
       "43606  Analyzing AI and student responses through the...   \n",
       "\n",
       "                                      doi    year journal  char_count  \\\n",
       "0                       10.1119/1.2715425     NaN     tpt        7234   \n",
       "1                       10.1119/1.2343976     NaN     tpt        4633   \n",
       "2                       10.1119/1.4745686     NaN     tpt       17496   \n",
       "3                       10.1119/1.2342910     NaN     tpt       37322   \n",
       "4                       10.1119/1.4865529     NaN     tpt        6621   \n",
       "...                                   ...     ...     ...         ...   \n",
       "43602       10.1119/perc.2023.pr.Willison  2023.0    perc       24881   \n",
       "43603             10.1119/perc.2023.pr.Wu  2023.0    perc       24460   \n",
       "43604          10.1119/perc.2023.pr.Young  2023.0    perc       27598   \n",
       "43605  10.1119/perc.2023.pr.Zohrabi_Alaee  2023.0    perc       27954   \n",
       "43606        10.1119/perc.2023.pr.Zollman  2023.0    perc       27691   \n",
       "\n",
       "       token_count                                          sentences  \\\n",
       "0          1808.50  [Make a Mystery Circuit with a Bar Light Fixtu...   \n",
       "1          1158.25  [AGOLDEN OLDIE-ABLAOK BOX OIROUIT \\r\\nClifton ...   \n",
       "2          4374.00  [Modeling Electricity: Model-Based Inquiry wit...   \n",
       "3          9330.50  [Two Approaches to Learning Physics \\r\\n\"I loo...   \n",
       "4          1655.25  [\\r\\nJochen Kuhn and Patrik Vogt, Column Edito...   \n",
       "...            ...                                                ...   \n",
       "43602      6220.25  [Examining faculty choices while implementing ...   \n",
       "43603      6115.00  [Analyzing the dimensionality of the Energy an...   \n",
       "43604      6899.50  [Students’ use of symmetry as a tool for sense...   \n",
       "43605      6988.50  [Analyzing Physics Majors’ Specialization Low ...   \n",
       "43606      6922.75  [Analyzing AI and student responses through th...   \n",
       "\n",
       "       sentences_count                                    sentence_chunks  \\\n",
       "0                   81  [[Make a Mystery Circuit with a Bar Light Fixt...   \n",
       "1                   58  [[AGOLDEN OLDIE-ABLAOK BOX OIROUIT \\r\\nClifton...   \n",
       "2                  140  [[Modeling Electricity: Model-Based Inquiry wi...   \n",
       "3                  324  [[Two Approaches to Learning Physics \\r\\n\"I lo...   \n",
       "4                   76  [[\\r\\nJochen Kuhn and Patrik Vogt, Column Edit...   \n",
       "...                ...                                                ...   \n",
       "43602              203  [[Examining faculty choices while implementing...   \n",
       "43603              227  [[Analyzing the dimensionality of the Energy a...   \n",
       "43604              264  [[Students’ use of symmetry as a tool for sens...   \n",
       "43605              222  [[Analyzing Physics Majors’ Specialization Low...   \n",
       "43606              219  [[Analyzing AI and student responses through t...   \n",
       "\n",
       "       num_chunks                                Full Text Embedding  \n",
       "0              17  [tensor(-0.0012), tensor(0.0354), tensor(0.108...  \n",
       "1              12  [tensor(0.0039), tensor(-0.0006), tensor(0.114...  \n",
       "2              28  [tensor(0.0564), tensor(-0.0816), tensor(0.150...  \n",
       "3              65  [tensor(0.0499), tensor(-0.1132), tensor(0.178...  \n",
       "4              16  [tensor(0.0292), tensor(-0.0519), tensor(0.055...  \n",
       "...           ...                                                ...  \n",
       "43602          41  [tensor(0.1635), tensor(-0.1263), tensor(0.057...  \n",
       "43603          46  [tensor(0.0763), tensor(-0.1726), tensor(0.054...  \n",
       "43604          53  [tensor(0.1260), tensor(-0.0853), tensor(0.105...  \n",
       "43605          45  [tensor(0.0705), tensor(-0.1829), tensor(0.015...  \n",
       "43606          44  [tensor(0.1471), tensor(-0.1589), tensor(0.174...  \n",
       "\n",
       "[43607 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text_and_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "940f85f9-9199-48ad-9884-bceab56bdf3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to get scores on 35376 embeddings: 0.02628 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Making a new dataframe for the whole-paper-embeddings to be inserted into\n",
    "\n",
    "df_labels_new=df_labels.merge(full_text_and_embeddings[['doi', 'Full Text Embedding']], how=\"left\", on=\"doi\")\n",
    "df_full_text=df_labels_new.copy()[['doi', 'year', 'title', 'journal','Full Text Embedding']]\n",
    "\n",
    "#Load the tensors onto the GPU for faster processing\n",
    "embeddings = torch.tensor(np.array(df_full_text['Full Text Embedding'].tolist()), dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "#Compute dot product similarity\n",
    "start_time = timer()\n",
    "dot_scores_1 = util.dot_score(a=query_embedding, b=embeddings).T  # Shape: (n_embeddings, 3)\n",
    "end_time = timer()\n",
    "\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "\n",
    "# Add columns for s1, s2, s3, s4 similarity to dataframe. Load onto the CPU for now\n",
    "for n, t in enumerate(query_list):\n",
    "    df_full_text[t]= dot_scores_1[:, n].cpu().numpy()\n",
    "\n",
    "\n",
    "#a = -10\n",
    "\n",
    "# Precompute the exponentials for efficiency\n",
    "exp_values = np.exp(a * (1-df_full_text[query_list]))\n",
    "\n",
    "# Compute the denominator for softmax-like normalization\n",
    "denominator = exp_values.sum(axis=1)\n",
    "\n",
    "# Create new score columns\n",
    "\n",
    "for col in query_list:\n",
    "    score_col = f\"{col.strip()}_score\"\n",
    "    df_full_text[score_col] = exp_values[col] / denominator\n",
    "\n",
    "# Step 2: Get main_group column based on max score\n",
    "score_cols = [f\"{col.strip()}_score\" for col in query_list]\n",
    "df_full_text['MG Score Full Embedding'] = df_full_text[score_cols].idxmax(axis=1).str.replace(\"_score\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a62ef9f2-be38-40c6-a2cc-a1038dadb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the two dataframes for simplicity \n",
    "df_labels_new=df_labels_new.merge(df_full_text[['doi', 'MG Score Full Embedding']], how=\"left\", on=\"doi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76d4a6-89ce-485d-a9ad-207d4d4cd9a5",
   "metadata": {},
   "source": [
    "##Evaluating the labeling process for chunked labels\n",
    "\n",
    "def evaluate_labeling(df, teacher_col, student_col, content_col, journal_col, main_group):\n",
    "    # Mapping of category labels to full column names\n",
    "        \"\"\"\n",
    "    Evaluate the accuracy of automated labeling by comparing against manual categorization.\n",
    "    \n",
    "    This function assesses how well an automated labeling system performs by comparing\n",
    "    its predictions against manual category labels. It provides detailed accuracy metrics\n",
    "    both overall and per category, along with label distribution statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing the data to evaluate. Must include columns:\n",
    "        - 'Category (M)': Manual category labels ('t', 's', 'c', 'jb', or variations)\n",
    "        - 'doi': Document identifiers for tracking results\n",
    "        - A column specified by `main_group` containing automated predictions\n",
    "    teacher_col : str\n",
    "        Full name/label corresponding to teacher category ('t')\n",
    "    student_col : str\n",
    "        Full name/label corresponding to student category ('s')  \n",
    "    content_col : str\n",
    "        Full name/label corresponding to content category ('c')\n",
    "    journal_col : str\n",
    "        Full name/label corresponding to journal/book category ('jb')\n",
    "    main_group : str\n",
    "        Column name in df containing the automated labeling predictions to evaluate\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with two columns:\n",
    "        - 'doi': Document identifiers from input data\n",
    "        - 'Correctly labeled': Boolean values indicating whether each entry\n",
    "          was correctly labeled by the automated system\n",
    "        Only includes rows that had valid manual categories.\n",
    "        \n",
    "    Side Effects\n",
    "    ------------\n",
    "    Prints comprehensive evaluation metrics to stdout:\n",
    "    1. Total number of entries with valid manual categories\n",
    "    2. Overall percentage of correctly labeled entries\n",
    "    3. Per-category accuracy breakdown\n",
    "    4. Label distribution for both manual and automated categories\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Manual category labels are case-insensitive and whitespace is stripped\n",
    "    - Entries with missing/NaN values in 'Category (M)' are excluded from evaluation\n",
    "    - Category mapping uses short codes: 't'->teacher, 's'->student, 'c'->content, 'jb'->journal\n",
    "    - Automated label names are truncated to 40 characters in output for readability\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    ... \n",
    "    >>> result = evaluate_labeling(df, 'Teacher Focus', 'Student Focus', \n",
    "    ...                           'Content Focus', 'Journal Business', \"MG Score Avg Score\")\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    category_map = {\n",
    "        't': teacher_col.strip(),\n",
    "        's': student_col.strip(),\n",
    "        'c': content_col.strip(),\n",
    "        'jb': journal_col.strip()\n",
    "    }\n",
    "\n",
    "    def compare_labels(row):\n",
    "        category = row['Category (M)']\n",
    "        if pd.isna(category):\n",
    "            return np.nan\n",
    "        expected_group = category_map.get(category.strip().lower())\n",
    "        return expected_group == row[main_group]\n",
    "\n",
    "    df['Correctly labeled'] = df.apply(compare_labels, axis=1)\n",
    "\n",
    "    # Only keep rows with a valid Category (M)\n",
    "    valid = df[~df['Category (M)'].isna()]\n",
    "\n",
    "    print(\"Here: \")\n",
    "    print(len(valid))\n",
    "\n",
    "    total = len(valid)\n",
    "    correct = valid['Correctly labeled'].sum()\n",
    "    percent_correct = 100 * correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"1. Total number of entries with a value in 'Category (M)': {total}\")\n",
    "    print(f\"2. Percent correctly labeled: {percent_correct:.2f}%\")\n",
    "\n",
    "    # 3. Per-category accuracy\n",
    "    print(\"3. Accuracy per category:\")\n",
    "    for cat, label in category_map.items():\n",
    "        group = valid[valid['Category (M)'].str.lower() == cat]\n",
    "        if not group.empty:\n",
    "            correct_in_group = group['Correctly labeled'].sum()\n",
    "            total_in_group = len(group)\n",
    "            frac = 100 * correct_in_group / total_in_group\n",
    "            print(f\"   {cat.title()}: {frac:.2f}%\")\n",
    "\n",
    "    # 4. Fraction of papers in each label\n",
    "    print(\"4. Label distribution:\")\n",
    "    cat_counts = valid['Category (M)'].str.lower().value_counts(normalize=True)\n",
    "    main_counts = valid[main_group].value_counts(normalize=True)\n",
    "    \n",
    "    print(\"Category (M):\")\n",
    "    for k, v in cat_counts.items():\n",
    "        print(f\"     {k.title()}: {v:.2%}\")\n",
    "    print(\"Automated:\")\n",
    "    for k, v in main_counts.items():\n",
    "        print(f\"     {k[:40].strip()}: {v:.2%}\")  # limit label length for neatness\n",
    "\n",
    "    # 5. Return doi + correctness list\n",
    "    result = valid[['doi', 'Correctly labeled']].reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "mg=\"MG Score Avg Score\"\n",
    "#mg=\"MG Score Full Embedding\"\n",
    "\n",
    "results = evaluate_labeling(\n",
    "    df_labels_new,\n",
    "    query_list[0], #teacher_col\n",
    "    query_list[1], #student_col\n",
    "    query_list[2], #content_col\n",
    "    query_list[3], #journal_col\n",
    "    mg)\n",
    "\n",
    "# View the returned doi + Correctly labeled result\n",
    "print(\"\\nReturned result (first few rows):\")\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e10804eb-2e99-497e-a7e5-c7126a52d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_labeling_recall(df, teacher_col, student_col, content_col, journal_col, main_group):\n",
    "    \"\"\"\n",
    "    Evaluate automated labeling performance with comprehensive classification metrics.\n",
    "    \n",
    "    This function provides an in-depth evaluation of automated labeling systems by\n",
    "    computing standard classification metrics including accuracy, precision, recall,\n",
    "    and false positive rates for each category. Unlike basic accuracy evaluation,\n",
    "    this function treats each category as a binary classification problem to provide\n",
    "    detailed per-class performance insights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing the data to evaluate. Must include columns:\n",
    "        - 'Category (M)': Manual category labels ('t', 's', 'c', 'jb', or variations)\n",
    "        - 'doi': Document identifiers for tracking results\n",
    "        - A column specified by `main_group` containing automated predictions\n",
    "    teacher_col : str\n",
    "        Full name/label corresponding to teacher category ('t')\n",
    "    student_col : str\n",
    "        Full name/label corresponding to student category ('s')  \n",
    "    content_col : str\n",
    "        Full name/label corresponding to content category ('c')\n",
    "    journal_col : str\n",
    "        Full name/label corresponding to journal business category ('jb')\n",
    "    main_group : str\n",
    "        Column name in df containing the automated labeling predictions to evaluate\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with two columns:\n",
    "        - 'doi': Document identifiers from input data\n",
    "        - 'Correctly labeled': Boolean values indicating whether each entry\n",
    "          was correctly labeled by the automated system\n",
    "        Only includes rows that had valid manual categories.\n",
    "        \n",
    "    Side Effects\n",
    "    ------------\n",
    "    Prints comprehensive evaluation metrics to stdout:\n",
    "    1. Total number of entries with valid manual categories\n",
    "    2. Overall accuracy percentage\n",
    "    3. Detailed per-category metrics:\n",
    "       - Recall (sensitivity): TP / (TP + FN) - ability to find all instances\n",
    "       - Precision: TP / (TP + FP) - accuracy of positive predictions\n",
    "       - False Positive Rate: FP / (FP + TN) - rate of incorrect positive predictions\n",
    "       - Category-specific accuracy: (TP + TN) / All - overall correctness for this category\n",
    "    4. Label distribution comparison between manual and automated classifications\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Manual category labels are case-insensitive and whitespace is stripped\n",
    "    - Entries with missing/NaN values in 'Category (M)' are excluded from evaluation\n",
    "    - Each category is evaluated as a binary classification problem (category vs. not-category)\n",
    "    - Category mapping uses short codes: 't'->teacher, 's'->student, 'c'->content, 'jb'->journal\n",
    "    - Automated label names are truncated to 40 characters in output for readability\n",
    "    - Confusion matrix elements (TP, TN, FP, FN) are computed for each category independently\n",
    "    \n",
    "    Classification Metrics Explained\n",
    "    --------------------------------\n",
    "    - **True Positives (TP)**: Correctly predicted as this category\n",
    "    - **False Negatives (FN)**: Actually this category but predicted as another\n",
    "    - **False Positives (FP)**: Predicted as this category but actually another\n",
    "    - **True Negatives (TN)**: Correctly predicted as NOT this category\n",
    "    - **Recall**: What fraction of actual instances were correctly identified?\n",
    "    - **Precision**: What fraction of positive predictions were correct?\n",
    "    - **False Positive Rate**: What fraction of negative cases were incorrectly flagged?\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    evaluate_labeling : Basic accuracy-only evaluation function\n",
    "    sklearn.metrics.classification_report : Similar comprehensive metrics from scikit-learn\n",
    "    \"\"\"\n",
    "    # Mapping of category labels to full column names\n",
    "    category_map = {\n",
    "        't': teacher_col.strip(),\n",
    "        's': student_col.strip(),\n",
    "        'c': content_col.strip(),\n",
    "        'jb': journal_col.strip()\n",
    "    }\n",
    "\n",
    "    def compare_labels(row):\n",
    "        category = row['Category (M)']\n",
    "        if pd.isna(category):\n",
    "            return np.nan\n",
    "        expected_group = category_map.get(category.strip().lower())\n",
    "        return expected_group == row[main_group]\n",
    "\n",
    "    df['Correctly labeled'] = df.apply(compare_labels, axis=1)\n",
    "    valid = df[~df['Category (M)'].isna()].copy()\n",
    "\n",
    "    total = len(valid)\n",
    "    correct = valid['Correctly labeled'].sum()\n",
    "    percent_correct = 100 * correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"1. Total number of entries with a value in 'Category (M)': {total}\")\n",
    "    print(f\"2. Percent correctly labeled (accuracy): {percent_correct:.2f}%\")\n",
    "\n",
    "    # Per-category accuracy (recall), precision, false positive rate\n",
    "    print(\"3. Detailed metrics per category:\")\n",
    "    for cat, expected_val in category_map.items():\n",
    "        # True Positives: predicted = expected = this category\n",
    "        tp = valid[(valid['Category (M)'].str.lower() == cat) & (valid[main_group] == expected_val)]\n",
    "        \n",
    "        # False Negatives: actual is this category, but predicted is not\n",
    "        fn = valid[(valid['Category (M)'].str.lower() == cat) & (valid[main_group] != expected_val)]\n",
    "\n",
    "        # False Positives: predicted is this category, but actual is not\n",
    "        fp = valid[(valid['Category (M)'].str.lower() != cat) & (valid[main_group] == expected_val)]\n",
    "\n",
    "        # True Negatives: actual and predicted are both *not* this category\n",
    "        tn = valid[(valid['Category (M)'].str.lower() != cat) & (valid[main_group] != expected_val)]\n",
    "\n",
    "        tp_count = len(tp)\n",
    "        fn_count = len(fn)\n",
    "        fp_count = len(fp)\n",
    "        tn_count = len(tn)\n",
    "\n",
    "        recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "        precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "        fpr = fp_count / (fp_count + tn_count) if (fp_count + tn_count) > 0 else 0\n",
    "        acc = (tp_count + tn_count) / (tp_count + tn_count + fp_count + fn_count)\n",
    "\n",
    "        print(f\"   {cat.title()}:\")\n",
    "        print(f\"      Recall (TP / TP + FN): {recall:.2f}\")\n",
    "        print(f\"      Precision (TP / TP + FP): {precision:.2f}\")\n",
    "        print(f\"      False Positive Rate (FP / FP + TN): {fpr:.2f}\")\n",
    "        print(f\"      Accuracy (TP + TN / All): {acc:.2f}\")\n",
    "\n",
    "    # Label distributions\n",
    "    print(\"4. Label distribution:\")\n",
    "    cat_counts = valid['Category (M)'].str.lower().value_counts(normalize=True)\n",
    "    main_counts = valid[main_group].value_counts(normalize=True)\n",
    "    \n",
    "    print(\"Category (M):\")\n",
    "    for k, v in cat_counts.items():\n",
    "        print(f\"     {k.title()}: {v:.2%}\")\n",
    "    print(\"Automated:\")\n",
    "    for k, v in main_counts.items():\n",
    "        print(f\"     {k[:40].strip()}: {v:.2%}\")\n",
    "\n",
    "    # Return doi and correctness\n",
    "    result = valid[['doi', 'Correctly labeled']].reset_index(drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2145b0a3-613e-4b03-b25b-488d57d0909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.ndimage as nd\n",
    "import numpy as np\n",
    "\n",
    "def centroid_labeling_sentences(df, K=1, J=1):\n",
    "    \"\"\"\n",
    "    Perform centroid-based topic labeling using sentence embeddings and archetypal papers.\n",
    "    \n",
    "    This function implements a sophisticated labeling approach that:\n",
    "    1. Identifies archetypal papers for each topic category\n",
    "    2. Extracts top-scoring sentences from these papers to create topic centroids\n",
    "    3. Compares all papers against these centroids using cosine similarity\n",
    "    4. Applies topic scoring to generate topic probabilities\n",
    "    5. Assigns the most likely topic label to each paper\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing papers to be labeled. Must include:\n",
    "        - 'doi': Document identifiers for each paper\n",
    "        - '{label}_score_weighted' columns for each label in query_list\n",
    "    K : int, optional\n",
    "        Number of top-scoring archetypal papers to use per topic category for \n",
    "        centroid creation (default: 1)\n",
    "    J : int, optional\n",
    "        Number of top-scoring sentence chunks to extract from each archetypal \n",
    "        paper for centroid calculation (default: 1)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Input dataframe enhanced with additional columns:\n",
    "        - 'Average Embeddings': Mean embedding vector for each paper\n",
    "        - '{label}_centroid_topic_score': Topic score probability scores for each topic\n",
    "        - 'Main Group Centroid': Predicted topic label (highest scoring category)\n",
    "        \n",
    "    Global Dependencies\n",
    "    -------------------\n",
    "    Requires the following global variables to be defined:\n",
    "    - query_list : list\n",
    "        List of topic labels/categories to classify papers into\n",
    "    - text_chunks_and_embeddings_df : pandas.DataFrame\n",
    "        Dataframe containing sentence-level data with columns:\n",
    "        - 'doi': Document identifiers matching those in input df\n",
    "        - 'embedding': Pre-computed sentence embedding vectors\n",
    "        - '{label}' columns: Sentence-level scores for each topic\n",
    "    - a : float\n",
    "        Scaling factor for softmax temperature (controls prediction confidence)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    - The function hard-codes archetypal papers for 'Journal business.' category\n",
    "    - Uses cosine similarity to measure distance between paper embeddings and centroids\n",
    "    - Applies topic score from Odden et al. 2024\n",
    "    - Papers with no embeddings receive zero-vectors as their average embedding\n",
    "    - Processing time is optimized using groupby operations for DOI lookups\n",
    "    \n",
    "    Algorithm Steps\n",
    "    ---------------\n",
    "    1. **Archetypal Selection**: Select top K papers per category based on weighted scores\n",
    "    2. **Centroid Creation**: Extract top J sentences from each archetypal paper and \n",
    "       average their embeddings to create topic centroids\n",
    "    3. **Paper Embedding**: Compute average embedding for each paper across all sentences\n",
    "    4. **Similarity Calculation**: Measure cosine similarity between paper embeddings \n",
    "       and topic centroids\n",
    "    5. **Topic Scoring**: Convert similarities to probabilities using the topic score equation from Odden et al. 2024\n",
    "    6. **Label Assignment**: Assign topic with highest probability as main group\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Assuming global variables are properly set up\n",
    "    >>> labeled_df = centroid_labeling_sentences(papers_df, K=5, J=10)\n",
    "    >>> print(labeled_df['Main Group Centroid'].value_counts())\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    cosine_similarity : Used for measuring embedding similarity\n",
    "    numpy.mean : Used for averaging embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    df_labels=df\n",
    "    print(f\"K: {K} J: {J}\") \n",
    "    \n",
    "    # Step 1: Identify archetypal papers\n",
    "    archetypal_papers = {}\n",
    "    for label in query_list:\n",
    "        top_k_dois = df_labels.sort_values(by=f\"{label}_score_weighted\", ascending=False).head(K)['doi'].tolist() #Here\n",
    "        archetypal_papers[label] = top_k_dois\n",
    "\n",
    "\n",
    "    # Step 2: Extract top sentences from archetypal papers\n",
    "    centroids = {}\n",
    "    for label, dois in archetypal_papers.items():\n",
    "        top_embeddings = []\n",
    "        for doi in dois:\n",
    "            rows = text_chunks_and_embeddings_df[text_chunks_and_embeddings_df['doi'] == doi].copy() #Removed the _score tag on the label because that doens't exist for that df\n",
    "            #print(rows.keys() )\n",
    "            rows_sorted = rows.sort_values(by=f\"{label}\", ascending=False)\n",
    "            top_j_embeddings = rows_sorted.head(J)['embedding'].tolist()\n",
    "            top_embeddings.extend(top_j_embeddings)\n",
    "        centroid = np.mean(np.stack(top_embeddings), axis=0)\n",
    "        centroids[label] = centroid\n",
    "    \n",
    "    # Step 3: Average embeddings for all papers\n",
    "    avg_embeddings = []\n",
    "    # Create a dictionary to speed up lookup by doi\n",
    "    doi_to_rows = text_chunks_and_embeddings_df.groupby('doi')\n",
    "    # Pre-group the dataframe by 'doi'\n",
    "    import time\n",
    "    start = time.perf_counter()\n",
    "    grouped_groupby = text_chunks_and_embeddings_df.groupby('doi')\n",
    "    avg_embeddings = []\n",
    "    for doi in df_labels['doi']:\n",
    "        rows = grouped_groupby.get_group(doi) if doi in grouped_groupby.groups else None\n",
    "    \n",
    "        if rows is not None:\n",
    "            embeddings_row = rows['embedding'].tolist()\n",
    "        else:\n",
    "            print(f\"This paper contained no embeddings: {doi}\")\n",
    "            embeddings_row = []\n",
    "    \n",
    "        if embeddings_row:\n",
    "            avg_embedding = np.mean(np.stack(embeddings_row), axis=0)\n",
    "        else:\n",
    "            avg_embedding = np.zeros_like(next(iter(centroids.values())))\n",
    "    \n",
    "        avg_embeddings.append(avg_embedding)\n",
    "    \n",
    "    #Average together all embeddings from each DOI and save as the centroid for that paper\n",
    "    df_labels['Average Embeddings'] = avg_embeddings\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    print(f\"Time taken: {end - start} seconds\")\n",
    "    \n",
    "    # Step 4: Cosine similarity to centroids\n",
    "    centroid_names = []\n",
    "    similarity_matrix = []\n",
    "    \n",
    "    for emb in df_labels['Average Embeddings']:\n",
    "        sims = [cosine_similarity(emb.reshape(1, -1), centroids[label].reshape(1, -1))[0][0] for label in query_list]\n",
    "        similarity_matrix.append(sims)\n",
    "    \n",
    "    similarity_matrix = np.array(similarity_matrix)\n",
    "    \n",
    "    # Step 5: Softmax scoring\n",
    "    exp_values = np.exp(a * (1 - similarity_matrix))\n",
    "    denominator = exp_values.sum(axis=1, keepdims=True)\n",
    "    topic_scores = exp_values / denominator\n",
    "    \n",
    "    # Step 6: Add topic scores to df_labels\n",
    "    for idx, label in enumerate(query_list):\n",
    "        colname = f\"{label}_centroid_topic_score\"\n",
    "        df_labels[colname] = topic_scores[:, idx]\n",
    "    \n",
    "    # Step 7: Identify main group\n",
    "    df_labels['Main Group Centroid'] = df_labels[[f\"{label}_centroid_topic_score\" for label in query_list]].idxmax(axis=1)\n",
    "    df_labels['Main Group Centroid'] = df_labels['Main Group Centroid'].str.replace(\"_centroid_topic_score\", \"\")\n",
    "    \n",
    "    return df_labels\n",
    "  \n",
    "#Calcualte the centroids for the methods that use whole text\n",
    "def centroid_labeling_whole_text(df, K=1, whole_text=True):\n",
    "    \"\"\"\n",
    "    Perform advanced centroid-based topic labeling using full-text embeddings.\n",
    "    \n",
    "    This function implements an enhanced centroid-based classification approach that:\n",
    "    1. Identifies archetypal papers for each topic category based on raw scores\n",
    "    2. Creates topic centroids from full-text embeddings of archetypal papers\n",
    "    3. Computes cosine similarity between all papers and topic centroids\n",
    "    4. Applies Topic Score to generate topic probabilities\n",
    "    5. Assigns the most likely topic label to each paper\n",
    "    \n",
    "    This approach differs from sentence-based methods by using document-level \n",
    "    embeddings, potentially capturing broader semantic themes and overall document structure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing papers to be labeled. Must include:\n",
    "        - 'doi': Document identifiers for each paper\n",
    "        - '{label}_score' columns for each label in query_list (raw scores)\n",
    "        - 'Full Text Embedding' column (when whole_text=True)\n",
    "    K : int, optional\n",
    "        Number of top-scoring archetypal papers to use per topic category for \n",
    "        centroid creation. Automatically adjusted if fewer papers are available\n",
    "        (default: 1)\n",
    "    whole_text : bool, optional\n",
    "        Whether to use full-text embeddings. Currently only True is supported\n",
    "        (default: True)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Input dataframe enhanced with additional columns:\n",
    "        - '{label}_Advanced Centroid dotp': Cosine similarity scores to each topic centroid\n",
    "        - '{label}_Advanced Centroid TS': Topic probabilities\n",
    "        - 'Main Group Advanced Centroid': Predicted topic label (highest scoring category)\n",
    "        \n",
    "    Global Dependencies\n",
    "    -------------------\n",
    "    Requires the following global variables to be defined:\n",
    "    - query_list : list\n",
    "        List of topic labels/categories to classify papers into\n",
    "    - df_full_text : pandas.DataFrame\n",
    "        Dataframe containing full-text embeddings with 'Full Text Embedding' column\n",
    "    - a : float\n",
    "        Temperature scaling factor for topic score (controls topic-mixedness)\n",
    "    - nd : module\n",
    "        Numerical computation module with rotate function (scipy.ndimage)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    - Uses raw scores ('{label}_score') rather than weighted scores for archetypal selection\n",
    "    - Automatically handles cases where K exceeds the number of available papers\n",
    "    - Cosine similarity is used instead of dot products for better normalized comparison\n",
    "    - The topic score transformation includes array rotation operations for proper alignment\n",
    "    - Currently only supports whole_text=True mode; sentence-level fallback not implemented\n",
    "    \n",
    "    Algorithm Steps\n",
    "    ---------------\n",
    "    1. **Archetypal Selection**: Select top K papers per category based on raw scores\n",
    "    2. **Centroid Creation**: Average full-text embeddings of archetypal papers \n",
    "       to create topic centroids\n",
    "    3. **Similarity Calculation**: Compute cosine similarity between all paper \n",
    "       embeddings and topic centroids\n",
    "    4. **Topic Scoring**: Topic score to convert similarities \n",
    "       to probabilities with array transformations\n",
    "    5. **Label Assignment**: Assign topic with highest probability as main group\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If whole_text=False (not currently supported)\n",
    "    IndexError\n",
    "        If required columns are missing from input dataframes\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Assuming global variables are properly set up\n",
    "    >>> labeled_df = advanced_centroid_labeling(papers_df, K=3, whole_text=True)\n",
    "    >>> print(labeled_df['Main Group Advanced Centroid'].value_counts())\n",
    "    >>> # Check similarity scores\n",
    "    >>> similarity_cols = [col for col in labeled_df.columns if 'dotp' in col]\n",
    "    >>> print(labeled_df[similarity_cols].describe())\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    cosine_similarity : Used for measuring embedding similarity\n",
    "    numpy.mean : Used for averaging embeddings\n",
    "    centroid_labeling_sentences : Alternative sentence-level approach\n",
    "    \"\"\"\n",
    "    df_labels = df.copy()\n",
    "    print(f\"K: {K} whole_text: {whole_text}\")\n",
    "\n",
    "    # Step 1: Identify archetypal papers\n",
    "    archetypal_papers = {}\n",
    "    for label in query_list:\n",
    "        k_actual = min(K, len(df_labels))\n",
    "        top_k_dois = df_labels.sort_values(by=f\"{label}_score\", ascending=False).head(k_actual)['doi'].tolist()\n",
    "        archetypal_papers[label] = top_k_dois\n",
    "    \n",
    "    # Step 2: Compute centroids\n",
    "    centroids = {}\n",
    "    for label, dois in archetypal_papers.items():\n",
    "        top_embeddings = []\n",
    "\n",
    "        for doi in dois:\n",
    "            if whole_text:\n",
    "                # Use full-text embedding directly\n",
    "                paper_embedding = df_labels[df_labels['doi'] == doi]['Full Text Embedding'].values\n",
    "                if len(paper_embedding) > 0:\n",
    "                    top_embeddings.append(paper_embedding[0])\n",
    "                else:\n",
    "                    print(\"There appears to be no full-text embeddings\")\n",
    "            centroid = np.mean(np.stack(top_embeddings), axis=0)\n",
    "            centroids[label] = centroid\n",
    "\n",
    "    # Step 3: Average embeddings per paper\n",
    "    avg_embeddings = []\n",
    "\n",
    "    # Step 4: Cosine similarity to centroids\n",
    "    dot_prods={}\n",
    "    for label in query_list:\n",
    "        new_label=label+\"_Advanced Centroid dotp\"\n",
    "        sent_emb=[item for item in df_full_text['Full Text Embedding']]\n",
    "        topic_emb=[centroids[label]]*len(sent_emb) #Copies the topic centroid embedding to be equal in length to sent_emb. For the case of the whole paper, this should just multiply it by one\n",
    "        dot_prods[new_label]= np.diag(np.array(cosine_similarity(topic_emb, sent_emb)))  ##This uses cosine similarity instead of dot product. The vectors used here are normalized so that isn't a problem\n",
    "    for new_label in [label+\"_Advanced Centroid dotp\" for label in query_list]:\n",
    "        df_labels[new_label]=dot_prods[new_label]\n",
    "    # Step 5: Topic Scoring \n",
    "    topic_scores=[]\n",
    "    ac_dotp_labels=[label+\"_Advanced Centroid dotp\" for label in query_list]\n",
    "    for index, row in df_labels.iterrows():\n",
    "        exp_values =np.exp(a* (1-row[ac_dotp_labels].values.astype(float)))\n",
    "        denominator = exp_values.sum(axis=0)\n",
    "        topic_scores.append( exp_values / denominator )\n",
    "    topic_scores=np.flip(nd.rotate(np.array(topic_scores),90), axis=0)\n",
    "    ac_ts_labels=[label+\"_Advanced Centroid TS\" for label in query_list]\n",
    "    for n, label in enumerate(ac_ts_labels):\n",
    "        df_labels[label]=topic_scores[n]\n",
    "    \n",
    "    # Step 7: Identify main group\n",
    "    df_labels['Main Group Advanced Centroid'] = df_labels[[f\"{label}_Advanced Centroid TS\" for label in query_list]].idxmax(axis=1)\n",
    "    df_labels['Main Group Advanced Centroid'] = df_labels['Main Group Advanced Centroid'].str.replace(\"_Advanced Centroid TS\", \"\")\n",
    "    \n",
    "    \n",
    "    return df_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58962fd4-fc0a-47e1-89a9-aade602c3b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 4 whole_text: True\n"
     ]
    }
   ],
   "source": [
    "whole_text_results=centroid_labeling_whole_text(df_full_text, K=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "082fd669-5250-4149-87e0-3089264c016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 4 J: 10\n",
      "Time taken: 19.379907619208097 seconds\n",
      "Here:\n",
      "94\n",
      "1. Total number of entries with a value in 'Category (M)': 94\n",
      "2. Percent correctly labeled (accuracy): 58.51%\n",
      "3. Detailed metrics per category:\n",
      "   T:\n",
      "      Recall (TP / TP + FN): 0.82\n",
      "      Precision (TP / TP + FP): 0.42\n",
      "      False Positive Rate (FP / FP + TN): 0.35\n",
      "      Accuracy (TP + TN / All): 0.69\n",
      "   S:\n",
      "      Recall (TP / TP + FN): 0.88\n",
      "      Precision (TP / TP + FP): 0.88\n",
      "      False Positive Rate (FP / FP + TN): 0.03\n",
      "      Accuracy (TP + TN / All): 0.96\n",
      "   C:\n",
      "      Recall (TP / TP + FN): 0.64\n",
      "      Precision (TP / TP + FP): 0.64\n",
      "      False Positive Rate (FP / FP + TN): 0.20\n",
      "      Accuracy (TP + TN / All): 0.74\n",
      "   Jb:\n",
      "      Recall (TP / TP + FN): 0.09\n",
      "      Precision (TP / TP + FP): 1.00\n",
      "      False Positive Rate (FP / FP + TN): 0.00\n",
      "      Accuracy (TP + TN / All): 0.78\n",
      "4. Label distribution:\n",
      "Category (M):\n",
      "     C: 35.11%\n",
      "     Jb: 24.47%\n",
      "     T: 23.40%\n",
      "     S: 17.02%\n",
      "Automated:\n",
      "     Teaching students.: 45.74%\n",
      "     Physics content.: 35.11%\n",
      "     Student focus.: 17.02%\n",
      "     Journal business.: 2.13%\n",
      "Here:\n",
      "94\n",
      "1. Total number of entries with a value in 'Category (M)': 94\n",
      "2. Percent correctly labeled (accuracy): 48.94%\n",
      "3. Detailed metrics per category:\n",
      "   T:\n",
      "      Recall (TP / TP + FN): 0.64\n",
      "      Precision (TP / TP + FP): 0.52\n",
      "      False Positive Rate (FP / FP + TN): 0.18\n",
      "      Accuracy (TP + TN / All): 0.78\n",
      "   S:\n",
      "      Recall (TP / TP + FN): 0.12\n",
      "      Precision (TP / TP + FP): 1.00\n",
      "      False Positive Rate (FP / FP + TN): 0.00\n",
      "      Accuracy (TP + TN / All): 0.85\n",
      "   C:\n",
      "      Recall (TP / TP + FN): 0.88\n",
      "      Precision (TP / TP + FP): 0.45\n",
      "      False Positive Rate (FP / FP + TN): 0.57\n",
      "      Accuracy (TP + TN / All): 0.59\n",
      "   Jb:\n",
      "      Recall (TP / TP + FN): 0.04\n",
      "      Precision (TP / TP + FP): 1.00\n",
      "      False Positive Rate (FP / FP + TN): 0.00\n",
      "      Accuracy (TP + TN / All): 0.77\n",
      "4. Label distribution:\n",
      "Category (M):\n",
      "     C: 35.11%\n",
      "     Jb: 24.47%\n",
      "     T: 23.40%\n",
      "     S: 17.02%\n",
      "Automated:\n",
      "     Physics content.: 68.09%\n",
      "     Teaching students.: 28.72%\n",
      "     Student focus.: 2.13%\n",
      "     Journal business.: 1.06%\n",
      "Here:\n",
      "94\n",
      "1. Total number of entries with a value in 'Category (M)': 94\n",
      "2. Percent correctly labeled (accuracy): 41.49%\n",
      "3. Detailed metrics per category:\n",
      "   T:\n",
      "      Recall (TP / TP + FN): 0.18\n",
      "      Precision (TP / TP + FP): 0.50\n",
      "      False Positive Rate (FP / FP + TN): 0.06\n",
      "      Accuracy (TP + TN / All): 0.77\n",
      "   S:\n",
      "      Recall (TP / TP + FN): 0.06\n",
      "      Precision (TP / TP + FP): 0.50\n",
      "      False Positive Rate (FP / FP + TN): 0.01\n",
      "      Accuracy (TP + TN / All): 0.83\n",
      "   C:\n",
      "      Recall (TP / TP + FN): 0.97\n",
      "      Precision (TP / TP + FP): 0.39\n",
      "      False Positive Rate (FP / FP + TN): 0.82\n",
      "      Accuracy (TP + TN / All): 0.46\n",
      "   Jb:\n",
      "      Recall (TP / TP + FN): 0.09\n",
      "      Precision (TP / TP + FP): 1.00\n",
      "      False Positive Rate (FP / FP + TN): 0.00\n",
      "      Accuracy (TP + TN / All): 0.78\n",
      "4. Label distribution:\n",
      "Category (M):\n",
      "     C: 35.11%\n",
      "     Jb: 24.47%\n",
      "     T: 23.40%\n",
      "     S: 17.02%\n",
      "Automated:\n",
      "     Physics content.: 87.23%\n",
      "     Teaching students.: 8.51%\n",
      "     Student focus.: 2.13%\n",
      "     Journal business.: 2.13%\n",
      "Here:\n",
      "94\n",
      "1. Total number of entries with a value in 'Category (M)': 94\n",
      "2. Percent correctly labeled (accuracy): 51.06%\n",
      "3. Detailed metrics per category:\n",
      "   T:\n",
      "      Recall (TP / TP + FN): 0.23\n",
      "      Precision (TP / TP + FP): 0.38\n",
      "      False Positive Rate (FP / FP + TN): 0.11\n",
      "      Accuracy (TP + TN / All): 0.73\n",
      "   S:\n",
      "      Recall (TP / TP + FN): 0.56\n",
      "      Precision (TP / TP + FP): 0.82\n",
      "      False Positive Rate (FP / FP + TN): 0.03\n",
      "      Accuracy (TP + TN / All): 0.90\n",
      "   C:\n",
      "      Recall (TP / TP + FN): 0.97\n",
      "      Precision (TP / TP + FP): 0.47\n",
      "      False Positive Rate (FP / FP + TN): 0.59\n",
      "      Accuracy (TP + TN / All): 0.61\n",
      "   Jb:\n",
      "      Recall (TP / TP + FN): 0.09\n",
      "      Precision (TP / TP + FP): 1.00\n",
      "      False Positive Rate (FP / FP + TN): 0.00\n",
      "      Accuracy (TP + TN / All): 0.78\n",
      "4. Label distribution:\n",
      "Category (M):\n",
      "     C: 35.11%\n",
      "     Jb: 24.47%\n",
      "     T: 23.40%\n",
      "     S: 17.02%\n",
      "Automated:\n",
      "     Physics content.: 72.34%\n",
      "     Teaching students.: 13.83%\n",
      "     Student focus.: 11.70%\n",
      "     Journal business.: 2.13%\n"
     ]
    }
   ],
   "source": [
    "df_full=whole_text_results.merge(df_labels_new[['doi', 'Category (M)']], how='left', on=\"doi\")\n",
    "df_labels_chunks=centroid_labeling_sentences(df_labels_new, K=4, J=10)\n",
    "\n",
    "\n",
    "#These are the labels for all the main groups. I apologize that these are all different labels, just poor coding on my part while I tried to make it more convenient\n",
    "mg1=\"Main Group Advanced Centroid\" #Advanced centroids based on full text\n",
    "mg2=\"MG Score Full Embedding\" #Non advanced centroid, labels from full text\n",
    "mg3=\"MG Score Avg Cos\" #Not advanced, Chunked\n",
    "mg4=\"Main Group Centroid\" #Advanced, chunked\n",
    "\n",
    "\n",
    "df1=df_full\n",
    "df2=df_labels_new\n",
    "df3=df_labels_new\n",
    "df4=df_labels_chunks\n",
    "\n",
    "##This is Group D, the one that matters\n",
    "results = evaluate_labeling_recall(\n",
    "    df1,\n",
    "    query_list[0], #teacher_col\n",
    "    query_list[1], #student_col\n",
    "    query_list[2], #content_col\n",
    "    query_list[3], #journal_col\n",
    "    mg1)\n",
    "\n",
    "results = evaluate_labeling_recall(\n",
    "    df2,\n",
    "    query_list[0], #teacher_col\n",
    "    query_list[1], #student_col\n",
    "    query_list[2], #content_col\n",
    "    query_list[3], #journal_col\n",
    "    mg2)\n",
    "\n",
    "\n",
    "results = evaluate_labeling_recall(\n",
    "    df3,\n",
    "    query_list[0], #teacher_col\n",
    "    query_list[1], #student_col\n",
    "    query_list[2], #content_col\n",
    "    query_list[3], #journal_col\n",
    "    mg3)\n",
    "\n",
    "\n",
    "results = evaluate_labeling_recall(\n",
    "    df4,\n",
    "    query_list[0], #teacher_col\n",
    "    query_list[1], #student_col\n",
    "    query_list[2], #content_col\n",
    "    query_list[3], #journal_col\n",
    "    mg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc87b40-53ff-4970-a5da-553322d9bce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7c6857f-4d1e-40cc-ba67-b8de8d296a7c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "for key in arch.keys():\n",
    "    print(f\"Subject: {key}\")\n",
    "    for item in arch[key]:\n",
    "        print(\"Item: \")\n",
    "        df_temp=df_full_text[df_full_text['doi']==item][['Teaching students._score', 'Student focus._score', 'Physics content._score', 'Journal business._score', 'doi', 'title']]\n",
    "        print(tabulate(df_temp, headers='keys'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0d543-62ce-4af5-87b6-c9c5f57a91dd",
   "metadata": {},
   "source": [
    "#Optionally save the dataframes to pickle files\n",
    "\n",
    "import pickle\n",
    "\n",
    "file_path=\"full_text_refined_df.pkl\"\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(df1, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_4",
   "language": "python",
   "name": "lang_4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
